{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewBX8llQIfoE"
   },
   "source": [
    "---\n",
    "Import, process, and clean the data\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eX_2Wj7Oh1ws"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Load in the data \n",
    "#df = pd.read_csv('rspct.tsv', sep=\"\\t\")\n",
    "df = pd.read_csv(r\"C:\\Users\\Z Dubs\\lambda\\sr_predict\\reddit_data\\rspct.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHTHRyxLogWu",
    "outputId": "e6b37f37-fd10-45d2-e406-c9950e952be2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1013000, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "y_o9iyQKorVI",
    "outputId": "b9636b50-6ef8-4b11-876c-985fea294b24"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "      <td>Remember your command line switches... Hi ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "      <td>So what was Matt \"addicted\" to? Did he ever sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>No Club Colors Funny story. I went to college ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "      <td>Not door bell, but floodlight mount height. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit                                              title  \\\n",
       "0  talesfromtechsupport             Remember your command line switches...   \n",
       "1               teenmom                    So what was Matt \"addicted\" to?   \n",
       "2                Harley                                     No Club Colors   \n",
       "3          ringdoorbell        Not door bell, but floodlight mount height.   \n",
       "4                 intel  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...   \n",
       "1  Did he ever say what his addiction was or is h...   \n",
       "2  Funny story. I went to college in Las Vegas. T...   \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...   \n",
       "4  Prime95 (regardless of version) and OCCT both,...   \n",
       "\n",
       "                                           full_text  \n",
       "0  Remember your command line switches... Hi ther...  \n",
       "1  So what was Matt \"addicted\" to? Did he ever sa...  \n",
       "2  No Club Colors Funny story. I went to college ...  \n",
       "3  Not door bell, but floodlight mount height. I ...  \n",
       "4  Worried about my 8700k small fft/data stress r...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of ambiguous data that can decrese model efficiency \n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "# Combine 'title' and 'selftext' columns to enrich data\n",
    "df[\"full_text\"] = df[\"title\"] + \" \" + df[\"selftext\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 5 percent of the dataset, even .1 of the data yields memory errors\n",
    "df = df.sample(frac=0.05, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# Function to clean html tags\n",
    "def soup(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>929565</th>\n",
       "      <td>OnePunchMan</td>\n",
       "      <td>[Spoilers] Black Sperm's cell stock</td>\n",
       "      <td>We all know Black Sperm is op, but let's revie...</td>\n",
       "      <td>[Spoilers] Black Sperm's cell stock We all kno...</td>\n",
       "      <td>[Spoilers] Black Sperm's cell stock We all kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100277</th>\n",
       "      <td>devops</td>\n",
       "      <td>PSA: Zookeeper 3.5 beta issues (can break Apac...</td>\n",
       "      <td>If anyone is looking into deploying Zookeeper ...</td>\n",
       "      <td>PSA: Zookeeper 3.5 beta issues (can break Apac...</td>\n",
       "      <td>PSA: Zookeeper 3.5 beta issues (can break Apac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707395</th>\n",
       "      <td>namenerds</td>\n",
       "      <td>How do you design a sibset?</td>\n",
       "      <td>Say you pick the first child's name, and it's ...</td>\n",
       "      <td>How do you design a sibset? Say you pick the f...</td>\n",
       "      <td>How do you design a sibset? Say you pick the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374556</th>\n",
       "      <td>twinpeaks</td>\n",
       "      <td>[S3E16] Plotlines still to be resolved before ...</td>\n",
       "      <td>With only two hours to go I wanted to get this...</td>\n",
       "      <td>[S3E16] Plotlines still to be resolved before ...</td>\n",
       "      <td>[S3E16] Plotlines still to be resolved before ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503596</th>\n",
       "      <td>FidgetSpinners</td>\n",
       "      <td>Why do teachers like to take Fidget Spinners?</td>\n",
       "      <td>I only see one situation. I can see when peopl...</td>\n",
       "      <td>Why do teachers like to take Fidget Spinners? ...</td>\n",
       "      <td>Why do teachers like to take Fidget Spinners? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                              title  \\\n",
       "929565     OnePunchMan                [Spoilers] Black Sperm's cell stock   \n",
       "100277          devops  PSA: Zookeeper 3.5 beta issues (can break Apac...   \n",
       "707395       namenerds                        How do you design a sibset?   \n",
       "374556       twinpeaks  [S3E16] Plotlines still to be resolved before ...   \n",
       "503596  FidgetSpinners      Why do teachers like to take Fidget Spinners?   \n",
       "\n",
       "                                                 selftext  \\\n",
       "929565  We all know Black Sperm is op, but let's revie...   \n",
       "100277  If anyone is looking into deploying Zookeeper ...   \n",
       "707395  Say you pick the first child's name, and it's ...   \n",
       "374556  With only two hours to go I wanted to get this...   \n",
       "503596  I only see one situation. I can see when peopl...   \n",
       "\n",
       "                                                full_text  \\\n",
       "929565  [Spoilers] Black Sperm's cell stock We all kno...   \n",
       "100277  PSA: Zookeeper 3.5 beta issues (can break Apac...   \n",
       "707395  How do you design a sibset? Say you pick the f...   \n",
       "374556  [S3E16] Plotlines still to be resolved before ...   \n",
       "503596  Why do teachers like to take Fidget Spinners? ...   \n",
       "\n",
       "                                               clean_text  \n",
       "929565  [Spoilers] Black Sperm's cell stock We all kno...  \n",
       "100277  PSA: Zookeeper 3.5 beta issues (can break Apac...  \n",
       "707395  How do you design a sibset? Say you pick the f...  \n",
       "374556  [S3E16] Plotlines still to be resolved before ...  \n",
       "503596  Why do teachers like to take Fidget Spinners? ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['full_text'].apply(soup)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lc8OO6RQE1Kf",
    "outputId": "a79f6544-9d48-4448-b9d6-020f0b82da94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AskEconomics          79\n",
       "AcademicPsychology    70\n",
       "Sneakers              70\n",
       "adderall              69\n",
       "gravityfalls          68\n",
       "                      ..\n",
       "RWBY                  34\n",
       "latin                 33\n",
       "RocketLeague          33\n",
       "civilengineering      33\n",
       "CryptoKitties         31\n",
       "Name: subreddit, Length: 1013, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at value count of target \n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aIp06MuE6I4Q",
    "outputId": "78e8d797-b819-49d4-d359-10fc2597bf0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40520, 5), (10130, 5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We'll do a 80/20 train/test split and stratify the target \n",
    "train, test = train_test_split(df, test_size=0.2, stratify=df[\"subreddit\"])\n",
    "\n",
    "# Sanity check \n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "cSm8J5ivCrSW",
    "outputId": "8faa556f-f2a6-4b53-f67e-1a820b0eb681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40520,)\n",
      "(10130,)\n",
      "(40520,)\n",
      "(10130,)\n"
     ]
    }
   ],
   "source": [
    "# Assign data to features/target\n",
    "X_train = train[\"clean_text\"]\n",
    "X_test = test[\"clean_text\"]\n",
    "\n",
    "y_train = train[\"subreddit\"]\n",
    "y_test = test[\"subreddit\"]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "0Do2sLnEKpA3",
    "outputId": "81cac967-225d-4ea9-a937-b099b20bbed8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50695         Epilepsy\n",
       "995704         amiugly\n",
       "225816             ftm\n",
       "848525    ringdoorbell\n",
       "724584         thesims\n",
       "Name: subreddit, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BZZ0M1piEW8s",
    "outputId": "b608c673-c2db-4090-ca93-7513100d4ca1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AskEconomics          63\n",
       "AcademicPsychology    56\n",
       "Sneakers              56\n",
       "adderall              55\n",
       "osugame               54\n",
       "                      ..\n",
       "RWBY                  27\n",
       "latin                 26\n",
       "RocketLeague          26\n",
       "civilengineering      26\n",
       "CryptoKitties         25\n",
       "Name: subreddit, Length: 1013, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check v2\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHlflpPEKbwc"
   },
   "source": [
    "---\n",
    "Use Label Encoding on the target \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONcy4S9cKbEG"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "# Fit the encoder\n",
    "encoder.fit(y_train)\n",
    "# Transform on train and test \n",
    "y_train = encoder.transform(y_train)\n",
    "y_test  = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Create a pipeline equipped with countvectorizer, tfidf-transformer, and a SGDClassifier model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([329])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_post = np.array([\"I love getting new sneakers; Jordan's, Nike, Addidas, custom footwear. Wanting to see what people think of these fresh kicks. I love the laces, contour of the shoe, more generic sneaker and shoe terms\"])\n",
    "text_clf.predict(test_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tattoo'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[329][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.90      0.75        10\n",
      "           1       0.60      0.38      0.46         8\n",
      "           2       0.62      0.50      0.56        10\n",
      "           3       0.71      0.56      0.63         9\n",
      "           4       0.83      0.45      0.59        11\n",
      "           5       0.91      0.83      0.87        12\n",
      "           6       0.75      0.50      0.60        12\n",
      "           7       0.52      1.00      0.69        11\n",
      "           8       0.91      0.91      0.91        11\n",
      "           9       0.50      0.56      0.53         9\n",
      "          10       0.57      0.40      0.47        10\n",
      "          11       0.69      0.92      0.79        12\n",
      "          12       0.54      0.78      0.64         9\n",
      "          13       0.58      0.50      0.54        14\n",
      "          14       0.50      0.67      0.57         9\n",
      "          15       0.56      0.50      0.53        10\n",
      "          16       0.80      0.89      0.84         9\n",
      "          17       0.50      0.78      0.61         9\n",
      "          18       1.00      0.70      0.82        10\n",
      "          19       0.67      0.36      0.47        11\n",
      "          20       0.62      0.56      0.59         9\n",
      "          21       0.83      0.50      0.62        10\n",
      "          22       0.67      0.77      0.71        13\n",
      "          23       0.60      0.75      0.67         8\n",
      "          24       0.60      0.30      0.40        10\n",
      "          25       0.60      0.82      0.69        11\n",
      "          26       0.64      0.82      0.72        11\n",
      "          27       0.67      0.22      0.33         9\n",
      "          28       0.61      0.88      0.72        16\n",
      "          29       0.23      0.30      0.26        10\n",
      "          30       0.50      0.36      0.42        11\n",
      "          31       0.90      0.90      0.90        10\n",
      "          32       0.73      0.80      0.76        10\n",
      "          33       0.50      0.40      0.44        10\n",
      "          34       1.00      0.67      0.80         9\n",
      "          35       0.91      0.83      0.87        12\n",
      "          36       0.60      1.00      0.75        12\n",
      "          37       1.00      0.33      0.50         9\n",
      "          38       0.71      1.00      0.83        12\n",
      "          39       0.44      0.78      0.56         9\n",
      "          40       0.73      0.73      0.73        11\n",
      "          41       0.62      0.56      0.59         9\n",
      "          42       0.75      0.75      0.75         8\n",
      "          43       0.64      0.75      0.69        12\n",
      "          44       0.88      0.78      0.82         9\n",
      "          45       0.60      0.60      0.60        10\n",
      "          46       0.67      0.67      0.67        12\n",
      "          47       0.67      0.91      0.77        11\n",
      "          48       0.77      1.00      0.87        10\n",
      "          49       0.89      0.89      0.89         9\n",
      "          50       0.50      0.38      0.43         8\n",
      "          51       0.57      0.80      0.67        10\n",
      "          52       0.53      0.73      0.62        11\n",
      "          53       0.71      0.56      0.63         9\n",
      "          54       0.82      0.90      0.86        10\n",
      "          55       1.00      0.29      0.44         7\n",
      "          56       0.70      0.88      0.78         8\n",
      "          57       1.00      0.70      0.82        10\n",
      "          58       0.55      0.55      0.55        11\n",
      "          59       0.62      1.00      0.76         8\n",
      "          60       0.73      0.80      0.76        10\n",
      "          61       0.80      0.40      0.53        10\n",
      "          62       0.80      0.80      0.80        10\n",
      "          63       0.64      0.88      0.74         8\n",
      "          64       0.80      0.40      0.53        10\n",
      "          65       0.67      0.44      0.53         9\n",
      "          66       0.33      0.22      0.27         9\n",
      "          67       0.75      0.60      0.67        10\n",
      "          68       0.69      1.00      0.81        11\n",
      "          69       0.53      0.89      0.67         9\n",
      "          70       0.60      0.60      0.60        10\n",
      "          71       1.00      0.88      0.93         8\n",
      "          72       0.73      0.80      0.76        10\n",
      "          73       0.88      0.70      0.78        10\n",
      "          74       0.67      0.89      0.76         9\n",
      "          75       0.71      0.83      0.77        12\n",
      "          76       0.58      0.70      0.64        10\n",
      "          77       0.43      0.43      0.43        14\n",
      "          78       0.71      0.45      0.56        11\n",
      "          79       0.56      0.56      0.56         9\n",
      "          80       0.50      0.50      0.50         8\n",
      "          81       0.67      0.67      0.67         9\n",
      "          82       0.62      0.73      0.67        11\n",
      "          83       0.57      0.44      0.50         9\n",
      "          84       0.69      1.00      0.82         9\n",
      "          85       0.75      0.50      0.60         6\n",
      "          86       0.60      0.90      0.72        10\n",
      "          87       0.88      0.78      0.82         9\n",
      "          88       1.00      0.29      0.44         7\n",
      "          89       0.86      0.60      0.71        10\n",
      "          90       0.80      0.80      0.80        10\n",
      "          91       0.67      0.55      0.60        11\n",
      "          92       0.67      0.40      0.50        10\n",
      "          93       0.50      0.50      0.50        12\n",
      "          94       0.73      0.92      0.81        12\n",
      "          95       1.00      0.82      0.90        11\n",
      "          96       0.78      0.70      0.74        10\n",
      "          97       0.38      0.33      0.35         9\n",
      "          98       0.86      0.67      0.75         9\n",
      "          99       0.67      0.75      0.71         8\n",
      "         100       0.71      0.91      0.80        11\n",
      "         101       0.73      0.92      0.81        12\n",
      "         102       0.86      0.92      0.89        13\n",
      "         103       0.67      0.50      0.57         8\n",
      "         104       0.71      1.00      0.83        10\n",
      "         105       0.50      0.18      0.27        11\n",
      "         106       0.18      0.38      0.24         8\n",
      "         107       0.59      0.83      0.69        12\n",
      "         108       0.67      0.50      0.57         8\n",
      "         109       1.00      0.70      0.82        10\n",
      "         110       0.67      0.55      0.60        11\n",
      "         111       0.82      0.82      0.82        11\n",
      "         112       1.00      0.80      0.89        10\n",
      "         113       0.58      0.58      0.58        12\n",
      "         114       0.82      0.90      0.86        10\n",
      "         115       1.00      0.82      0.90        11\n",
      "         116       0.78      0.58      0.67        12\n",
      "         117       0.67      0.44      0.53         9\n",
      "         118       0.83      0.91      0.87        11\n",
      "         119       0.89      0.89      0.89         9\n",
      "         120       1.00      0.50      0.67         8\n",
      "         121       0.71      0.91      0.80        11\n",
      "         122       0.64      0.78      0.70         9\n",
      "         123       0.67      0.67      0.67        12\n",
      "         124       0.88      0.64      0.74        11\n",
      "         125       0.75      1.00      0.86         9\n",
      "         126       1.00      0.80      0.89        10\n",
      "         127       0.88      0.70      0.78        10\n",
      "         128       0.79      0.92      0.85        12\n",
      "         129       0.78      0.64      0.70        11\n",
      "         130       0.25      0.11      0.15         9\n",
      "         131       0.64      0.58      0.61        12\n",
      "         132       0.56      0.50      0.53        10\n",
      "         133       0.65      1.00      0.79        11\n",
      "         134       0.58      0.85      0.69        13\n",
      "         135       0.75      0.90      0.82        10\n",
      "         136       0.71      0.92      0.80        13\n",
      "         137       0.73      1.00      0.85        11\n",
      "         138       0.62      0.56      0.59         9\n",
      "         139       0.71      0.56      0.63         9\n",
      "         140       0.80      0.40      0.53        10\n",
      "         141       0.89      0.80      0.84        10\n",
      "         142       0.71      0.91      0.80        11\n",
      "         143       0.71      0.50      0.59        10\n",
      "         144       0.78      0.78      0.78         9\n",
      "         145       0.75      0.38      0.50         8\n",
      "         146       0.50      0.56      0.53         9\n",
      "         147       1.00      0.20      0.33        10\n",
      "         148       0.60      0.60      0.60        10\n",
      "         149       0.88      0.64      0.74        11\n",
      "         150       0.56      0.62      0.59         8\n",
      "         151       0.90      0.90      0.90        10\n",
      "         152       0.53      0.80      0.64        10\n",
      "         153       1.00      0.25      0.40         8\n",
      "         154       0.75      0.38      0.50         8\n",
      "         155       0.67      0.29      0.40         7\n",
      "         156       1.00      0.88      0.93         8\n",
      "         157       0.86      0.60      0.71        10\n",
      "         158       0.75      0.43      0.55         7\n",
      "         159       0.64      0.90      0.75        10\n",
      "         160       0.53      0.80      0.64        10\n",
      "         161       0.83      0.56      0.67         9\n",
      "         162       0.91      0.91      0.91        11\n",
      "         163       0.56      0.50      0.53        10\n",
      "         164       0.50      0.14      0.22         7\n",
      "         165       0.62      0.89      0.73         9\n",
      "         166       0.83      0.56      0.67         9\n",
      "         167       0.67      0.92      0.77        13\n",
      "         168       1.00      0.30      0.46        10\n",
      "         169       0.82      1.00      0.90         9\n",
      "         170       0.64      0.64      0.64        11\n",
      "         171       0.40      0.73      0.52        11\n",
      "         172       0.89      0.73      0.80        11\n",
      "         173       0.67      0.89      0.76         9\n",
      "         174       0.73      0.80      0.76        10\n",
      "         175       1.00      1.00      1.00         9\n",
      "         176       0.71      0.50      0.59        10\n",
      "         177       0.71      0.45      0.56        11\n",
      "         178       0.60      0.55      0.57        11\n",
      "         179       0.36      0.56      0.43         9\n",
      "         180       0.67      0.80      0.73        10\n",
      "         181       0.67      1.00      0.80        10\n",
      "         182       0.73      0.89      0.80         9\n",
      "         183       0.73      0.89      0.80         9\n",
      "         184       0.50      0.33      0.40         9\n",
      "         185       0.40      0.73      0.52        11\n",
      "         186       0.50      0.50      0.50         8\n",
      "         187       0.80      0.50      0.62         8\n",
      "         188       0.58      0.70      0.64        10\n",
      "         189       0.75      0.55      0.63        11\n",
      "         190       0.67      0.73      0.70        11\n",
      "         191       0.86      0.67      0.75         9\n",
      "         192       0.73      0.80      0.76        10\n",
      "         193       0.67      0.44      0.53         9\n",
      "         194       1.00      1.00      1.00        10\n",
      "         195       0.78      0.70      0.74        10\n",
      "         196       0.88      0.78      0.82         9\n",
      "         197       0.85      1.00      0.92        11\n",
      "         198       0.67      0.75      0.71         8\n",
      "         199       1.00      0.38      0.55         8\n",
      "         200       1.00      0.82      0.90        11\n",
      "         201       0.88      0.70      0.78        10\n",
      "         202       0.58      0.70      0.64        10\n",
      "         203       0.73      1.00      0.85        11\n",
      "         204       0.60      0.75      0.67        12\n",
      "         205       0.56      0.56      0.56         9\n",
      "         206       0.77      0.91      0.83        11\n",
      "         207       0.59      0.83      0.69        12\n",
      "         208       0.50      0.67      0.57         9\n",
      "         209       0.60      0.75      0.67         8\n",
      "         210       0.70      0.64      0.67        11\n",
      "         211       0.44      0.64      0.52        11\n",
      "         212       0.00      0.00      0.00         9\n",
      "         213       0.71      0.62      0.67         8\n",
      "         214       0.78      0.78      0.78         9\n",
      "         215       1.00      1.00      1.00        10\n",
      "         216       1.00      0.22      0.36         9\n",
      "         217       0.75      0.67      0.71         9\n",
      "         218       0.83      0.62      0.71         8\n",
      "         219       0.22      0.25      0.24         8\n",
      "         220       0.86      0.86      0.86         7\n",
      "         221       0.62      0.73      0.67        11\n",
      "         222       0.88      0.78      0.82         9\n",
      "         223       0.54      0.64      0.58        11\n",
      "         224       0.56      0.82      0.67        11\n",
      "         225       0.80      0.40      0.53        10\n",
      "         226       0.60      0.90      0.72        10\n",
      "         227       1.00      0.30      0.46        10\n",
      "         228       0.64      0.58      0.61        12\n",
      "         229       0.60      0.30      0.40        10\n",
      "         230       0.83      0.62      0.71         8\n",
      "         231       0.88      0.78      0.82         9\n",
      "         232       0.67      0.73      0.70        11\n",
      "         233       0.62      0.89      0.73         9\n",
      "         234       0.69      1.00      0.81        11\n",
      "         235       0.60      0.90      0.72        10\n",
      "         236       0.75      0.33      0.46         9\n",
      "         237       0.46      1.00      0.63        12\n",
      "         238       1.00      0.60      0.75        10\n",
      "         239       1.00      0.56      0.71         9\n",
      "         240       0.70      0.58      0.64        12\n",
      "         241       0.38      0.27      0.32        11\n",
      "         242       1.00      1.00      1.00         9\n",
      "         243       0.64      0.64      0.64        11\n",
      "         244       0.76      1.00      0.87        13\n",
      "         245       0.71      1.00      0.83        10\n",
      "         246       0.86      0.67      0.75         9\n",
      "         247       0.61      0.92      0.73        12\n",
      "         248       0.56      0.42      0.48        12\n",
      "         249       0.67      0.89      0.76         9\n",
      "         250       0.80      0.80      0.80        10\n",
      "         251       0.80      0.44      0.57         9\n",
      "         252       0.77      0.91      0.83        11\n",
      "         253       0.33      0.43      0.38         7\n",
      "         254       0.62      0.50      0.56        10\n",
      "         255       1.00      0.57      0.73         7\n",
      "         256       0.83      0.45      0.59        11\n",
      "         257       0.75      0.82      0.78        11\n",
      "         258       0.78      0.70      0.74        10\n",
      "         259       0.78      0.58      0.67        12\n",
      "         260       0.62      0.89      0.73         9\n",
      "         261       1.00      0.71      0.83         7\n",
      "         262       0.86      0.75      0.80         8\n",
      "         263       0.62      0.56      0.59         9\n",
      "         264       0.42      0.45      0.43        11\n",
      "         265       0.55      0.55      0.55        11\n",
      "         266       0.83      1.00      0.91        10\n",
      "         267       0.50      0.36      0.42        11\n",
      "         268       0.69      0.82      0.75        11\n",
      "         269       0.86      0.67      0.75         9\n",
      "         270       0.67      0.67      0.67         9\n",
      "         271       0.62      0.80      0.70        10\n",
      "         272       1.00      0.89      0.94         9\n",
      "         273       0.50      0.75      0.60        12\n",
      "         274       0.57      0.89      0.70         9\n",
      "         275       0.56      0.56      0.56         9\n",
      "         276       0.70      0.78      0.74         9\n",
      "         277       0.57      0.67      0.62        12\n",
      "         278       0.33      0.14      0.20         7\n",
      "         279       0.73      0.92      0.81        12\n",
      "         280       1.00      0.27      0.43        11\n",
      "         281       0.54      0.64      0.58        11\n",
      "         282       0.64      0.78      0.70         9\n",
      "         283       0.25      0.12      0.17         8\n",
      "         284       0.27      0.36      0.31        11\n",
      "         285       0.67      0.67      0.67         9\n",
      "         286       0.70      0.78      0.74         9\n",
      "         287       0.62      1.00      0.77        10\n",
      "         288       0.77      0.91      0.83        11\n",
      "         289       0.62      0.71      0.67         7\n",
      "         290       0.65      0.85      0.73        13\n",
      "         291       0.80      0.36      0.50        11\n",
      "         292       0.33      0.22      0.27         9\n",
      "         293       0.57      0.44      0.50         9\n",
      "         294       0.67      0.80      0.73        10\n",
      "         295       0.33      0.29      0.31         7\n",
      "         296       1.00      0.56      0.71         9\n",
      "         297       0.85      0.92      0.88        12\n",
      "         298       0.50      0.44      0.47         9\n",
      "         299       0.89      0.80      0.84        10\n",
      "         300       1.00      0.50      0.67        10\n",
      "         301       1.00      0.90      0.95        10\n",
      "         302       0.69      0.82      0.75        11\n",
      "         303       1.00      0.29      0.44         7\n",
      "         304       0.90      0.90      0.90        10\n",
      "         305       0.54      0.78      0.64         9\n",
      "         306       0.75      0.50      0.60        12\n",
      "         307       0.73      0.73      0.73        11\n",
      "         308       0.67      0.91      0.77        11\n",
      "         309       0.77      0.83      0.80        12\n",
      "         310       0.67      0.67      0.67        12\n",
      "         311       0.42      0.50      0.45        10\n",
      "         312       0.45      0.90      0.60        10\n",
      "         313       0.89      0.80      0.84        10\n",
      "         314       0.60      0.27      0.37        11\n",
      "         315       0.67      0.50      0.57        12\n",
      "         316       0.50      0.67      0.57         9\n",
      "         317       0.71      0.91      0.80        11\n",
      "         318       0.67      0.36      0.47        11\n",
      "         319       0.67      0.55      0.60        11\n",
      "         320       1.00      0.70      0.82        10\n",
      "         321       0.82      0.82      0.82        11\n",
      "         322       0.50      0.64      0.56        11\n",
      "         323       0.62      0.91      0.74        11\n",
      "         324       0.83      0.56      0.67         9\n",
      "         325       0.83      0.91      0.87        11\n",
      "         326       0.89      0.73      0.80        11\n",
      "         327       0.85      1.00      0.92        11\n",
      "         328       0.60      0.90      0.72        10\n",
      "         329       0.43      0.71      0.54        14\n",
      "         330       0.91      1.00      0.95        10\n",
      "         331       0.75      0.60      0.67        10\n",
      "         332       0.73      0.89      0.80         9\n",
      "         333       0.86      0.86      0.86         7\n",
      "         334       0.50      0.83      0.62        12\n",
      "         335       0.67      0.83      0.74        12\n",
      "         336       0.33      0.22      0.27         9\n",
      "         337       0.69      0.69      0.69        13\n",
      "         338       0.60      0.60      0.60        10\n",
      "         339       0.64      0.78      0.70         9\n",
      "         340       1.00      0.27      0.43        11\n",
      "         341       0.58      0.92      0.71        12\n",
      "         342       0.50      0.57      0.53         7\n",
      "         343       0.64      0.88      0.74         8\n",
      "         344       0.64      0.64      0.64        11\n",
      "         345       0.67      0.50      0.57         8\n",
      "         346       0.82      0.90      0.86        10\n",
      "         347       0.75      0.67      0.71         9\n",
      "         348       0.82      1.00      0.90         9\n",
      "         349       0.75      0.75      0.75        12\n",
      "         350       0.82      0.90      0.86        10\n",
      "         351       0.71      0.83      0.77        12\n",
      "         352       0.47      0.78      0.58         9\n",
      "         353       0.88      1.00      0.93         7\n",
      "         354       0.56      0.50      0.53        10\n",
      "         355       0.67      0.50      0.57         8\n",
      "         356       0.75      0.50      0.60        12\n",
      "         357       0.83      0.62      0.71         8\n",
      "         358       0.69      0.82      0.75        11\n",
      "         359       0.61      0.92      0.73        12\n",
      "         360       1.00      0.67      0.80         9\n",
      "         361       0.55      0.55      0.55        11\n",
      "         362       0.78      0.58      0.67        12\n",
      "         363       0.73      0.73      0.73        11\n",
      "         364       0.79      1.00      0.88        11\n",
      "         365       0.67      0.50      0.57         8\n",
      "         366       0.82      0.69      0.75        13\n",
      "         367       0.73      0.92      0.81        12\n",
      "         368       1.00      0.67      0.80         9\n",
      "         369       0.60      0.60      0.60        10\n",
      "         370       0.75      0.67      0.71         9\n",
      "         371       0.38      0.38      0.38         8\n",
      "         372       0.86      0.92      0.89        13\n",
      "         373       0.50      0.78      0.61         9\n",
      "         374       0.75      0.90      0.82        10\n",
      "         375       1.00      0.88      0.93         8\n",
      "         376       0.64      0.90      0.75        10\n",
      "         377       0.88      0.64      0.74        11\n",
      "         378       0.67      0.80      0.73        10\n",
      "         379       0.33      0.12      0.18         8\n",
      "         380       0.56      0.75      0.64        12\n",
      "         381       0.73      0.73      0.73        11\n",
      "         382       0.91      1.00      0.95        10\n",
      "         383       1.00      0.50      0.67        12\n",
      "         384       0.65      1.00      0.79        11\n",
      "         385       0.67      0.91      0.77        11\n",
      "         386       0.67      0.40      0.50        10\n",
      "         387       0.00      0.00      0.00        10\n",
      "         388       0.60      0.60      0.60        10\n",
      "         389       0.89      0.80      0.84        10\n",
      "         390       0.86      0.60      0.71        10\n",
      "         391       0.78      0.64      0.70        11\n",
      "         392       0.67      0.40      0.50        10\n",
      "         393       1.00      0.62      0.77         8\n",
      "         394       0.71      0.56      0.63         9\n",
      "         395       0.69      0.75      0.72        12\n",
      "         396       0.62      0.45      0.53        11\n",
      "         397       0.56      0.62      0.59         8\n",
      "         398       1.00      0.44      0.62         9\n",
      "         399       0.83      0.50      0.62        10\n",
      "         400       0.73      0.89      0.80         9\n",
      "         401       0.89      0.89      0.89         9\n",
      "         402       0.85      1.00      0.92        11\n",
      "         403       0.64      1.00      0.78         9\n",
      "         404       0.80      0.89      0.84         9\n",
      "         405       0.39      0.64      0.48        11\n",
      "         406       0.55      0.55      0.55        11\n",
      "         407       0.58      0.70      0.64        10\n",
      "         408       0.80      0.89      0.84         9\n",
      "         409       0.43      0.30      0.35        10\n",
      "         410       0.62      0.56      0.59         9\n",
      "         411       1.00      0.88      0.93         8\n",
      "         412       0.71      0.50      0.59        10\n",
      "         413       0.62      0.67      0.64        12\n",
      "         414       0.69      0.90      0.78        10\n",
      "         415       0.75      0.50      0.60        12\n",
      "         416       0.36      0.36      0.36        11\n",
      "         417       0.40      0.20      0.27        10\n",
      "         418       0.83      0.91      0.87        11\n",
      "         419       0.75      0.86      0.80        14\n",
      "         420       0.71      0.83      0.77        12\n",
      "         421       0.70      0.64      0.67        11\n",
      "         422       0.70      0.70      0.70        10\n",
      "         423       0.78      0.64      0.70        11\n",
      "         424       0.62      1.00      0.76         8\n",
      "         425       0.57      0.40      0.47        10\n",
      "         426       0.70      0.70      0.70        10\n",
      "         427       0.73      0.80      0.76        10\n",
      "         428       0.75      1.00      0.86        12\n",
      "         429       0.67      0.80      0.73        10\n",
      "         430       0.67      0.50      0.57        12\n",
      "         431       0.57      0.50      0.53         8\n",
      "         432       0.86      0.75      0.80         8\n",
      "         433       0.56      0.75      0.64        12\n",
      "         434       0.75      1.00      0.86         9\n",
      "         435       1.00      0.89      0.94         9\n",
      "         436       0.55      0.67      0.60         9\n",
      "         437       0.82      0.82      0.82        11\n",
      "         438       0.71      1.00      0.83        10\n",
      "         439       0.80      0.33      0.47        12\n",
      "         440       0.75      0.92      0.83        13\n",
      "         441       0.80      0.80      0.80        10\n",
      "         442       0.80      0.89      0.84         9\n",
      "         443       0.33      0.12      0.18         8\n",
      "         444       0.38      0.30      0.33        10\n",
      "         445       0.75      1.00      0.86         9\n",
      "         446       0.55      0.60      0.57        10\n",
      "         447       0.67      0.60      0.63        10\n",
      "         448       0.67      0.83      0.74        12\n",
      "         449       0.67      0.67      0.67         9\n",
      "         450       0.58      0.92      0.71        12\n",
      "         451       0.50      0.33      0.40         9\n",
      "         452       0.57      0.50      0.53         8\n",
      "         453       0.75      0.67      0.71         9\n",
      "         454       0.89      0.80      0.84        10\n",
      "         455       0.70      0.70      0.70        10\n",
      "         456       0.50      0.64      0.56        11\n",
      "         457       0.57      0.50      0.53         8\n",
      "         458       0.71      0.50      0.59        10\n",
      "         459       0.33      0.11      0.17         9\n",
      "         460       0.75      0.38      0.50         8\n",
      "         461       0.83      0.50      0.62        10\n",
      "         462       0.67      0.44      0.53         9\n",
      "         463       0.60      1.00      0.75         9\n",
      "         464       0.78      0.64      0.70        11\n",
      "         465       0.64      0.64      0.64        11\n",
      "         466       0.70      0.64      0.67        11\n",
      "         467       0.44      0.50      0.47         8\n",
      "         468       0.70      0.88      0.78         8\n",
      "         469       0.67      0.44      0.53         9\n",
      "         470       0.40      0.22      0.29         9\n",
      "         471       0.44      0.58      0.50        12\n",
      "         472       0.78      0.88      0.82         8\n",
      "         473       0.64      0.70      0.67        10\n",
      "         474       0.60      0.69      0.64        13\n",
      "         475       0.90      0.75      0.82        12\n",
      "         476       0.17      0.11      0.13         9\n",
      "         477       0.62      0.56      0.59         9\n",
      "         478       0.67      0.91      0.77        11\n",
      "         479       0.44      0.40      0.42        10\n",
      "         480       1.00      0.50      0.67        10\n",
      "         481       0.78      0.64      0.70        11\n",
      "         482       0.80      0.44      0.57         9\n",
      "         483       0.75      0.60      0.67        10\n",
      "         484       0.50      0.67      0.57         9\n",
      "         485       0.71      0.62      0.67         8\n",
      "         486       0.88      0.64      0.74        11\n",
      "         487       0.83      1.00      0.91        10\n",
      "         488       0.71      0.42      0.53        12\n",
      "         489       0.91      1.00      0.95        10\n",
      "         490       0.92      1.00      0.96        11\n",
      "         491       0.50      0.27      0.35        11\n",
      "         492       0.73      0.80      0.76        10\n",
      "         493       0.67      0.60      0.63        10\n",
      "         494       0.88      0.70      0.78        10\n",
      "         495       0.67      0.57      0.62         7\n",
      "         496       0.80      0.80      0.80        10\n",
      "         497       0.73      1.00      0.85        11\n",
      "         498       0.60      0.67      0.63         9\n",
      "         499       0.75      0.75      0.75         8\n",
      "         500       0.55      0.60      0.57        10\n",
      "         501       0.55      0.67      0.60         9\n",
      "         502       0.44      0.33      0.38        12\n",
      "         503       0.78      0.88      0.82         8\n",
      "         504       1.00      0.83      0.91        12\n",
      "         505       0.73      1.00      0.85        11\n",
      "         506       0.75      0.90      0.82        10\n",
      "         507       0.80      0.89      0.84         9\n",
      "         508       1.00      0.70      0.82        10\n",
      "         509       1.00      0.56      0.71         9\n",
      "         510       0.75      1.00      0.86         9\n",
      "         511       0.45      0.62      0.53         8\n",
      "         512       0.67      0.25      0.36         8\n",
      "         513       0.64      0.90      0.75        10\n",
      "         514       1.00      0.36      0.53        11\n",
      "         515       0.83      0.50      0.62        10\n",
      "         516       0.91      0.83      0.87        12\n",
      "         517       0.50      0.11      0.18         9\n",
      "         518       0.83      1.00      0.91        10\n",
      "         519       0.50      0.57      0.53         7\n",
      "         520       0.50      0.33      0.40         9\n",
      "         521       0.67      0.73      0.70        11\n",
      "         522       0.69      1.00      0.82         9\n",
      "         523       0.50      0.30      0.37        10\n",
      "         524       0.65      1.00      0.79        11\n",
      "         525       0.73      0.80      0.76        10\n",
      "         526       0.71      1.00      0.83        12\n",
      "         527       0.71      0.56      0.63         9\n",
      "         528       0.83      0.62      0.71         8\n",
      "         529       0.57      0.50      0.53         8\n",
      "         530       0.60      0.60      0.60        10\n",
      "         531       0.69      0.75      0.72        12\n",
      "         532       0.80      0.33      0.47        12\n",
      "         533       0.86      0.50      0.63        12\n",
      "         534       0.67      0.22      0.33         9\n",
      "         535       0.00      0.00      0.00         9\n",
      "         536       0.60      0.90      0.72        10\n",
      "         537       0.71      0.50      0.59        10\n",
      "         538       0.89      1.00      0.94         8\n",
      "         539       0.77      1.00      0.87        10\n",
      "         540       0.54      0.70      0.61        10\n",
      "         541       0.75      0.60      0.67        10\n",
      "         542       0.60      0.60      0.60        10\n",
      "         543       0.73      0.89      0.80         9\n",
      "         544       0.70      0.78      0.74         9\n",
      "         545       0.75      0.67      0.71         9\n",
      "         546       0.60      0.67      0.63         9\n",
      "         547       0.75      0.25      0.38        12\n",
      "         548       0.90      0.90      0.90        10\n",
      "         549       0.67      0.50      0.57        12\n",
      "         550       0.67      0.91      0.77        11\n",
      "         551       0.62      0.80      0.70        10\n",
      "         552       0.64      1.00      0.78         7\n",
      "         553       0.77      1.00      0.87        10\n",
      "         554       0.43      0.30      0.35        10\n",
      "         555       0.53      0.83      0.65        12\n",
      "         556       0.56      0.56      0.56         9\n",
      "         557       0.67      0.75      0.71         8\n",
      "         558       0.71      0.62      0.67         8\n",
      "         559       0.47      0.64      0.54        11\n",
      "         560       0.70      0.70      0.70        10\n",
      "         561       0.67      0.36      0.47        11\n",
      "         562       1.00      0.78      0.88         9\n",
      "         563       0.52      0.92      0.67        13\n",
      "         564       0.67      0.73      0.70        11\n",
      "         565       0.78      0.78      0.78         9\n",
      "         566       0.58      0.64      0.61        11\n",
      "         567       0.62      0.50      0.56        10\n",
      "         568       0.58      0.58      0.58        12\n",
      "         569       0.62      0.80      0.70        10\n",
      "         570       0.88      0.64      0.74        11\n",
      "         571       1.00      0.22      0.36         9\n",
      "         572       0.43      0.27      0.33        11\n",
      "         573       0.83      1.00      0.91        10\n",
      "         574       0.83      0.56      0.67         9\n",
      "         575       0.71      0.50      0.59        10\n",
      "         576       0.60      0.30      0.40        10\n",
      "         577       0.00      0.00      0.00        12\n",
      "         578       0.80      0.89      0.84         9\n",
      "         579       0.55      0.67      0.60         9\n",
      "         580       0.62      0.45      0.53        11\n",
      "         581       0.67      0.89      0.76         9\n",
      "         582       0.82      0.90      0.86        10\n",
      "         583       0.73      0.92      0.81        12\n",
      "         584       0.67      0.25      0.36         8\n",
      "         585       0.69      0.75      0.72        12\n",
      "         586       0.63      1.00      0.77        12\n",
      "         587       0.54      0.78      0.64         9\n",
      "         588       0.83      0.56      0.67         9\n",
      "         589       0.92      0.92      0.92        12\n",
      "         590       0.90      0.90      0.90        10\n",
      "         591       0.42      0.42      0.42        12\n",
      "         592       0.50      0.67      0.57         9\n",
      "         593       0.73      0.89      0.80         9\n",
      "         594       0.75      0.90      0.82        10\n",
      "         595       0.88      0.78      0.82         9\n",
      "         596       0.64      0.82      0.72        11\n",
      "         597       0.89      0.89      0.89         9\n",
      "         598       0.67      0.89      0.76         9\n",
      "         599       0.53      0.80      0.64        10\n",
      "         600       0.78      0.78      0.78         9\n",
      "         601       0.67      0.50      0.57         8\n",
      "         602       0.73      1.00      0.85        11\n",
      "         603       0.83      0.62      0.71         8\n",
      "         604       0.89      1.00      0.94         8\n",
      "         605       1.00      0.50      0.67        10\n",
      "         606       0.77      0.91      0.83        11\n",
      "         607       0.58      0.70      0.64        10\n",
      "         608       0.54      0.88      0.67         8\n",
      "         609       0.78      0.78      0.78         9\n",
      "         610       0.50      0.10      0.17        10\n",
      "         611       0.58      0.64      0.61        11\n",
      "         612       1.00      0.64      0.78        11\n",
      "         613       0.70      0.78      0.74         9\n",
      "         614       0.90      0.75      0.82        12\n",
      "         615       0.79      0.92      0.85        12\n",
      "         616       0.00      0.00      0.00         8\n",
      "         617       0.44      0.40      0.42        10\n",
      "         618       0.62      0.56      0.59         9\n",
      "         619       0.60      0.60      0.60        10\n",
      "         620       0.70      0.64      0.67        11\n",
      "         621       0.67      0.80      0.73        10\n",
      "         622       0.61      1.00      0.76        11\n",
      "         623       0.67      0.46      0.55        13\n",
      "         624       0.50      0.70      0.58        10\n",
      "         625       0.58      1.00      0.73        11\n",
      "         626       0.82      0.82      0.82        11\n",
      "         627       0.50      0.38      0.43         8\n",
      "         628       0.25      0.12      0.17         8\n",
      "         629       0.44      0.50      0.47         8\n",
      "         630       0.59      0.71      0.65        14\n",
      "         631       0.70      0.70      0.70        10\n",
      "         632       0.83      0.91      0.87        11\n",
      "         633       0.60      0.27      0.37        11\n",
      "         634       0.91      0.91      0.91        11\n",
      "         635       0.80      0.92      0.86        13\n",
      "         636       0.80      0.73      0.76        11\n",
      "         637       1.00      0.10      0.18        10\n",
      "         638       0.50      0.60      0.55        10\n",
      "         639       0.69      0.90      0.78        10\n",
      "         640       0.91      1.00      0.95        10\n",
      "         641       0.75      0.90      0.82        10\n",
      "         642       0.78      0.78      0.78         9\n",
      "         643       0.80      0.73      0.76        11\n",
      "         644       0.57      0.40      0.47        10\n",
      "         645       0.90      0.82      0.86        11\n",
      "         646       0.50      0.50      0.50        10\n",
      "         647       0.57      0.44      0.50         9\n",
      "         648       0.50      0.10      0.17        10\n",
      "         649       1.00      0.60      0.75        10\n",
      "         650       0.78      0.70      0.74        10\n",
      "         651       0.80      0.44      0.57         9\n",
      "         652       0.67      0.73      0.70        11\n",
      "         653       0.55      0.75      0.63         8\n",
      "         654       0.75      0.33      0.46         9\n",
      "         655       0.50      0.67      0.57         9\n",
      "         656       0.43      0.30      0.35        10\n",
      "         657       0.70      0.70      0.70        10\n",
      "         658       0.85      0.92      0.88        12\n",
      "         659       0.67      0.44      0.53         9\n",
      "         660       0.57      0.73      0.64        11\n",
      "         661       0.67      0.50      0.57         8\n",
      "         662       0.50      0.12      0.20         8\n",
      "         663       0.61      0.85      0.71        13\n",
      "         664       0.50      0.40      0.44        10\n",
      "         665       0.57      0.36      0.44        11\n",
      "         666       0.57      0.40      0.47        10\n",
      "         667       0.41      0.64      0.50        11\n",
      "         668       0.75      1.00      0.86         9\n",
      "         669       0.44      0.78      0.56         9\n",
      "         670       0.42      0.50      0.45        10\n",
      "         671       0.64      0.70      0.67        10\n",
      "         672       0.70      0.78      0.74         9\n",
      "         673       0.00      0.00      0.00        10\n",
      "         674       0.62      0.56      0.59         9\n",
      "         675       0.70      0.88      0.78         8\n",
      "         676       0.88      0.88      0.88         8\n",
      "         677       0.50      1.00      0.67        10\n",
      "         678       0.73      0.80      0.76        10\n",
      "         679       0.69      1.00      0.82         9\n",
      "         680       0.62      0.45      0.53        11\n",
      "         681       0.71      0.83      0.77        12\n",
      "         682       0.73      1.00      0.85        11\n",
      "         683       0.33      0.20      0.25        10\n",
      "         684       0.83      0.62      0.71         8\n",
      "         685       0.86      1.00      0.92        12\n",
      "         686       1.00      0.82      0.90        11\n",
      "         687       1.00      0.50      0.67         8\n",
      "         688       0.60      0.82      0.69        11\n",
      "         689       0.88      0.88      0.88         8\n",
      "         690       0.50      0.14      0.22         7\n",
      "         691       0.62      0.91      0.74        11\n",
      "         692       0.70      0.70      0.70        10\n",
      "         693       0.58      0.70      0.64        10\n",
      "         694       0.62      0.50      0.56        10\n",
      "         695       0.41      0.58      0.48        12\n",
      "         696       0.00      0.00      0.00         8\n",
      "         697       0.89      0.73      0.80        11\n",
      "         698       1.00      0.09      0.17        11\n",
      "         699       0.71      0.56      0.63         9\n",
      "         700       0.88      0.88      0.88         8\n",
      "         701       0.71      0.50      0.59        10\n",
      "         702       1.00      0.40      0.57        10\n",
      "         703       0.71      1.00      0.83        10\n",
      "         704       0.70      0.70      0.70        10\n",
      "         705       0.67      0.67      0.67         9\n",
      "         706       0.55      0.67      0.60         9\n",
      "         707       0.80      0.73      0.76        11\n",
      "         708       0.50      0.10      0.17        10\n",
      "         709       0.75      1.00      0.86        12\n",
      "         710       0.55      1.00      0.71        12\n",
      "         711       0.56      0.45      0.50        11\n",
      "         712       0.50      0.22      0.31         9\n",
      "         713       0.64      0.82      0.72        11\n",
      "         714       0.91      0.91      0.91        11\n",
      "         715       0.62      0.45      0.53        11\n",
      "         716       0.67      0.55      0.60        11\n",
      "         717       0.56      0.91      0.69        11\n",
      "         718       0.36      0.36      0.36        11\n",
      "         719       0.62      0.73      0.67        11\n",
      "         720       1.00      0.22      0.36         9\n",
      "         721       0.69      0.75      0.72        12\n",
      "         722       0.75      0.82      0.78        11\n",
      "         723       1.00      0.91      0.95        11\n",
      "         724       0.57      0.44      0.50         9\n",
      "         725       0.71      1.00      0.83        10\n",
      "         726       0.86      0.60      0.71        10\n",
      "         727       0.60      0.27      0.37        11\n",
      "         728       0.80      0.73      0.76        11\n",
      "         729       0.62      0.62      0.62         8\n",
      "         730       0.64      0.90      0.75        10\n",
      "         731       0.57      0.73      0.64        11\n",
      "         732       0.53      0.83      0.65        12\n",
      "         733       0.53      0.69      0.60        13\n",
      "         734       0.57      0.73      0.64        11\n",
      "         735       0.43      0.60      0.50        10\n",
      "         736       0.69      0.82      0.75        11\n",
      "         737       1.00      0.60      0.75        10\n",
      "         738       0.53      0.89      0.67         9\n",
      "         739       0.73      0.80      0.76        10\n",
      "         740       1.00      0.62      0.77         8\n",
      "         741       0.50      0.33      0.40         9\n",
      "         742       0.88      0.78      0.82         9\n",
      "         743       0.63      0.92      0.75        13\n",
      "         744       1.00      0.30      0.46        10\n",
      "         745       0.60      0.60      0.60        10\n",
      "         746       0.33      0.25      0.29         8\n",
      "         747       0.67      0.80      0.73        10\n",
      "         748       0.62      0.56      0.59         9\n",
      "         749       0.56      0.69      0.62        13\n",
      "         750       0.50      0.50      0.50        12\n",
      "         751       0.50      0.45      0.48        11\n",
      "         752       0.58      0.88      0.70         8\n",
      "         753       0.56      0.82      0.67        11\n",
      "         754       0.80      0.44      0.57         9\n",
      "         755       0.83      0.62      0.71         8\n",
      "         756       0.80      0.50      0.62         8\n",
      "         757       0.25      0.11      0.15         9\n",
      "         758       0.56      0.69      0.62        13\n",
      "         759       0.73      0.80      0.76        10\n",
      "         760       0.42      0.50      0.45        10\n",
      "         761       0.80      0.57      0.67        14\n",
      "         762       0.75      0.27      0.40        11\n",
      "         763       0.83      0.56      0.67         9\n",
      "         764       0.62      0.56      0.59         9\n",
      "         765       0.80      0.73      0.76        11\n",
      "         766       0.86      0.55      0.67        11\n",
      "         767       0.56      0.90      0.69        10\n",
      "         768       0.57      1.00      0.73         8\n",
      "         769       1.00      0.55      0.71        11\n",
      "         770       0.88      0.88      0.88         8\n",
      "         771       0.50      0.10      0.17        10\n",
      "         772       0.73      0.73      0.73        11\n",
      "         773       0.00      0.00      0.00         9\n",
      "         774       1.00      0.89      0.94         9\n",
      "         775       0.92      1.00      0.96        12\n",
      "         776       0.43      0.33      0.38         9\n",
      "         777       0.42      0.45      0.43        11\n",
      "         778       0.67      0.40      0.50        10\n",
      "         779       0.88      0.64      0.74        11\n",
      "         780       0.82      1.00      0.90         9\n",
      "         781       0.70      0.70      0.70        10\n",
      "         782       0.83      0.50      0.62        10\n",
      "         783       1.00      0.67      0.80         9\n",
      "         784       1.00      0.62      0.77         8\n",
      "         785       1.00      0.25      0.40        12\n",
      "         786       0.69      0.90      0.78        10\n",
      "         787       0.62      0.80      0.70        10\n",
      "         788       0.62      0.73      0.67        11\n",
      "         789       0.86      0.60      0.71        10\n",
      "         790       0.70      0.70      0.70        10\n",
      "         791       0.67      0.86      0.75         7\n",
      "         792       1.00      0.12      0.22         8\n",
      "         793       0.67      0.80      0.73        10\n",
      "         794       0.58      0.70      0.64        10\n",
      "         795       1.00      0.75      0.86        12\n",
      "         796       0.75      0.55      0.63        11\n",
      "         797       0.20      0.11      0.14         9\n",
      "         798       0.80      0.36      0.50        11\n",
      "         799       0.42      0.92      0.58        12\n",
      "         800       0.80      0.44      0.57         9\n",
      "         801       0.43      0.67      0.52         9\n",
      "         802       0.64      1.00      0.78         9\n",
      "         803       0.62      0.77      0.69        13\n",
      "         804       0.17      0.11      0.13         9\n",
      "         805       0.75      1.00      0.86         9\n",
      "         806       0.88      0.64      0.74        11\n",
      "         807       0.86      0.55      0.67        11\n",
      "         808       0.60      0.33      0.43         9\n",
      "         809       0.45      0.45      0.45        11\n",
      "         810       0.73      1.00      0.85        11\n",
      "         811       0.71      0.83      0.77        12\n",
      "         812       0.53      1.00      0.70         8\n",
      "         813       0.77      0.91      0.83        11\n",
      "         814       0.56      0.90      0.69        10\n",
      "         815       0.67      0.25      0.36         8\n",
      "         816       0.89      0.89      0.89         9\n",
      "         817       0.80      0.40      0.53        10\n",
      "         818       0.86      0.75      0.80         8\n",
      "         819       0.89      0.80      0.84        10\n",
      "         820       0.60      0.60      0.60        10\n",
      "         821       0.61      1.00      0.76        11\n",
      "         822       0.89      0.80      0.84        10\n",
      "         823       0.83      0.62      0.71         8\n",
      "         824       0.60      0.67      0.63         9\n",
      "         825       0.88      0.88      0.88         8\n",
      "         826       0.67      0.44      0.53         9\n",
      "         827       0.62      0.89      0.73         9\n",
      "         828       0.70      0.88      0.78         8\n",
      "         829       0.62      0.50      0.56        10\n",
      "         830       0.71      0.56      0.63         9\n",
      "         831       0.62      1.00      0.77        10\n",
      "         832       0.71      0.91      0.80        11\n",
      "         833       0.50      0.22      0.31         9\n",
      "         834       1.00      0.67      0.80        12\n",
      "         835       0.55      0.67      0.60         9\n",
      "         836       0.80      0.80      0.80        10\n",
      "         837       0.50      0.75      0.60         8\n",
      "         838       0.42      0.42      0.42        12\n",
      "         839       0.89      0.89      0.89         9\n",
      "         840       0.62      0.56      0.59         9\n",
      "         841       0.54      0.64      0.58        11\n",
      "         842       0.56      0.56      0.56         9\n",
      "         843       0.73      0.67      0.70        12\n",
      "         844       0.45      0.91      0.61        11\n",
      "         845       0.75      0.60      0.67        10\n",
      "         846       1.00      0.12      0.22         8\n",
      "         847       0.60      0.90      0.72        10\n",
      "         848       0.39      0.85      0.54        13\n",
      "         849       0.83      0.62      0.71         8\n",
      "         850       0.58      0.54      0.56        13\n",
      "         851       0.38      0.27      0.32        11\n",
      "         852       0.82      0.82      0.82        11\n",
      "         853       0.50      0.50      0.50        10\n",
      "         854       0.57      0.89      0.70         9\n",
      "         855       0.64      0.64      0.64        11\n",
      "         856       0.67      1.00      0.80        10\n",
      "         857       0.64      0.75      0.69        12\n",
      "         858       0.90      1.00      0.95         9\n",
      "         859       0.60      0.75      0.67        12\n",
      "         860       0.00      0.00      0.00         9\n",
      "         861       0.80      1.00      0.89         8\n",
      "         862       0.71      0.45      0.56        11\n",
      "         863       1.00      0.86      0.92         7\n",
      "         864       0.82      0.75      0.78        12\n",
      "         865       0.57      0.73      0.64        11\n",
      "         866       0.83      1.00      0.91        10\n",
      "         867       0.75      1.00      0.86         9\n",
      "         868       0.61      0.92      0.73        12\n",
      "         869       0.75      0.75      0.75        12\n",
      "         870       0.86      0.60      0.71        10\n",
      "         871       0.62      0.91      0.74        11\n",
      "         872       0.86      0.67      0.75         9\n",
      "         873       1.00      0.25      0.40         8\n",
      "         874       0.67      0.50      0.57         8\n",
      "         875       0.44      0.44      0.44         9\n",
      "         876       0.33      0.40      0.36        10\n",
      "         877       0.36      0.36      0.36        11\n",
      "         878       0.67      0.50      0.57         8\n",
      "         879       0.43      0.90      0.58        10\n",
      "         880       0.60      0.90      0.72        10\n",
      "         881       0.54      0.78      0.64         9\n",
      "         882       0.00      0.00      0.00        10\n",
      "         883       0.80      0.57      0.67         7\n",
      "         884       0.60      0.27      0.37        11\n",
      "         885       0.46      0.60      0.52        10\n",
      "         886       0.64      0.82      0.72        11\n",
      "         887       0.88      0.64      0.74        11\n",
      "         888       0.89      0.80      0.84        10\n",
      "         889       0.50      0.58      0.54        12\n",
      "         890       0.70      0.78      0.74         9\n",
      "         891       0.67      0.60      0.63        10\n",
      "         892       1.00      0.67      0.80         9\n",
      "         893       0.86      0.55      0.67        11\n",
      "         894       0.50      0.67      0.57         9\n",
      "         895       0.50      0.25      0.33         8\n",
      "         896       0.82      0.90      0.86        10\n",
      "         897       0.80      0.73      0.76        11\n",
      "         898       0.64      0.78      0.70         9\n",
      "         899       0.67      0.80      0.73        10\n",
      "         900       0.69      0.90      0.78        10\n",
      "         901       1.00      0.67      0.80         9\n",
      "         902       0.60      0.55      0.57        11\n",
      "         903       0.86      0.60      0.71        10\n",
      "         904       0.78      0.78      0.78         9\n",
      "         905       0.50      0.29      0.36         7\n",
      "         906       0.67      0.60      0.63        10\n",
      "         907       0.50      0.64      0.56        11\n",
      "         908       0.38      0.27      0.32        11\n",
      "         909       0.67      0.60      0.63        10\n",
      "         910       0.78      0.78      0.78         9\n",
      "         911       0.57      0.40      0.47        10\n",
      "         912       0.55      1.00      0.71        11\n",
      "         913       0.71      0.56      0.63         9\n",
      "         914       0.69      0.75      0.72        12\n",
      "         915       0.62      0.80      0.70        10\n",
      "         916       0.45      0.50      0.48        10\n",
      "         917       0.86      0.67      0.75         9\n",
      "         918       0.33      0.57      0.42         7\n",
      "         919       0.67      1.00      0.80        10\n",
      "         920       0.69      0.82      0.75        11\n",
      "         921       1.00      0.36      0.53        11\n",
      "         922       0.56      0.82      0.67        11\n",
      "         923       0.80      0.80      0.80        10\n",
      "         924       0.56      0.91      0.69        11\n",
      "         925       0.50      1.00      0.67        10\n",
      "         926       0.42      0.62      0.50         8\n",
      "         927       0.70      0.78      0.74         9\n",
      "         928       0.57      0.89      0.70         9\n",
      "         929       0.50      0.50      0.50        10\n",
      "         930       0.40      0.17      0.24        12\n",
      "         931       1.00      0.64      0.78        11\n",
      "         932       0.55      0.92      0.69        12\n",
      "         933       0.70      0.70      0.70        10\n",
      "         934       0.77      1.00      0.87        10\n",
      "         935       0.86      0.75      0.80         8\n",
      "         936       1.00      0.89      0.94         9\n",
      "         937       0.83      0.50      0.62        10\n",
      "         938       0.53      0.80      0.64        10\n",
      "         939       1.00      0.91      0.95        11\n",
      "         940       0.80      0.80      0.80        10\n",
      "         941       0.86      0.75      0.80         8\n",
      "         942       0.00      0.00      0.00         9\n",
      "         943       0.78      0.64      0.70        11\n",
      "         944       0.62      1.00      0.77        10\n",
      "         945       0.75      0.67      0.71         9\n",
      "         946       0.80      0.50      0.62         8\n",
      "         947       0.73      0.73      0.73        11\n",
      "         948       0.60      0.25      0.35        12\n",
      "         949       0.50      0.40      0.44        10\n",
      "         950       0.77      1.00      0.87        10\n",
      "         951       0.53      0.82      0.64        11\n",
      "         952       0.60      0.55      0.57        11\n",
      "         953       0.55      0.60      0.57        10\n",
      "         954       0.54      0.64      0.58        11\n",
      "         955       0.50      0.75      0.60        12\n",
      "         956       0.77      0.77      0.77        13\n",
      "         957       0.75      0.60      0.67        10\n",
      "         958       0.47      0.80      0.59        10\n",
      "         959       1.00      0.67      0.80         9\n",
      "         960       0.40      0.18      0.25        11\n",
      "         961       0.60      0.30      0.40        10\n",
      "         962       0.67      0.50      0.57        12\n",
      "         963       0.67      0.89      0.76         9\n",
      "         964       0.73      0.80      0.76        10\n",
      "         965       0.88      0.78      0.82         9\n",
      "         966       0.50      0.67      0.57         9\n",
      "         967       0.62      0.83      0.71        12\n",
      "         968       0.88      0.88      0.88         8\n",
      "         969       0.44      0.64      0.52        11\n",
      "         970       0.89      0.80      0.84        10\n",
      "         971       0.42      1.00      0.59        10\n",
      "         972       0.91      1.00      0.95        10\n",
      "         973       0.77      1.00      0.87        10\n",
      "         974       0.62      0.50      0.56        10\n",
      "         975       0.88      0.78      0.82         9\n",
      "         976       0.75      0.82      0.78        11\n",
      "         977       1.00      0.82      0.90        11\n",
      "         978       0.64      0.75      0.69        12\n",
      "         979       0.62      0.80      0.70        10\n",
      "         980       0.83      0.56      0.67         9\n",
      "         981       0.60      0.33      0.43         9\n",
      "         982       0.88      0.78      0.82         9\n",
      "         983       0.50      0.33      0.40         9\n",
      "         984       0.29      0.62      0.40         8\n",
      "         985       1.00      0.10      0.18        10\n",
      "         986       0.71      0.56      0.63         9\n",
      "         987       0.86      0.60      0.71        10\n",
      "         988       0.86      0.60      0.71        10\n",
      "         989       1.00      0.89      0.94         9\n",
      "         990       0.90      0.90      0.90        10\n",
      "         991       0.82      0.75      0.78        12\n",
      "         992       0.73      0.92      0.81        12\n",
      "         993       0.56      0.90      0.69        10\n",
      "         994       0.80      0.40      0.53        10\n",
      "         995       0.62      0.56      0.59         9\n",
      "         996       0.33      0.10      0.15        10\n",
      "         997       0.71      1.00      0.83        10\n",
      "         998       1.00      0.12      0.22         8\n",
      "         999       1.00      0.73      0.84        11\n",
      "        1000       0.58      0.78      0.67         9\n",
      "        1001       0.75      0.75      0.75        12\n",
      "        1002       0.44      0.44      0.44         9\n",
      "        1003       0.62      0.73      0.67        11\n",
      "        1004       0.40      0.29      0.33         7\n",
      "        1005       0.25      0.25      0.25        12\n",
      "        1006       0.50      0.11      0.18         9\n",
      "        1007       0.64      0.58      0.61        12\n",
      "        1008       1.00      0.82      0.90        11\n",
      "        1009       0.75      0.90      0.82        10\n",
      "        1010       0.75      1.00      0.86         9\n",
      "        1011       0.75      0.75      0.75         8\n",
      "        1012       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.67     10130\n",
      "   macro avg       0.69      0.66      0.65     10130\n",
      "weighted avg       0.68      0.67      0.65     10130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "text_clf.fit(X_train,y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5340078973346496\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_train_pred = text_clf.predict(X_train)\n",
    "print (metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Alright, we can make a prediction and see the top result, lets make a function that returns the top 5 results\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(post, num_answers=5):\n",
    "  \"\"\" takes a potential post and returns the top options \"\"\"\n",
    "\n",
    "  preds = pd.Series(text_clf.decision_function(post)[0])\n",
    "\n",
    "  preds.index = text_clf.classes_\n",
    "\n",
    "  preds = preds.sort_values(ascending=False)\n",
    "\n",
    "  return preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329   -0.401971\n",
       "248   -0.958687\n",
       "137   -0.985770\n",
       "647   -0.987266\n",
       "209   -0.989961\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(test_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit                                       DBZDokkanBattle\n",
       "title                               dokkan battlefield level 16\n",
       "selftext      how the fuck do you beat this level? ive sent ...\n",
       "full_text     dokkan battlefield level 16 how the fuck do yo...\n",
       "clean_text    dokkan battlefield level 16 how the fuck do yo...\n",
       "Name: 711479, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_post = [ \"\"\"\n",
    "               LeBron James and Kobe Bryant and both great NBA players and we should stop comparing them.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738   -0.870125\n",
       "982   -0.873794\n",
       "772   -0.933504\n",
       "457   -0.964201\n",
       "989   -0.967183\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(nba_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit                                                Ripple\n",
       "title                              Quick paper wallet questions\n",
       "selftext      I plan on buying Ripple soon and storing it in...\n",
       "full_text     Quick paper wallet questions I plan on buying ...\n",
       "clean_text    Quick paper wallet questions I plan on buying ...\n",
       "Name: 254102, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[738]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Testing pipeline with a couple other models really quickly, first RFC\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_clf1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "    ])\n",
    "\n",
    "text_clf1.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([329])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_post = np.array([\"I love getting new sneakers; Jordan's, Nike, Addidas, custom footwear. Wanting to see what people think of these fresh kicks\"])\n",
    "text_clf1.predict(test_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4590819348469891\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_train_pred1 = text_clf1.predict(X_train)\n",
    "print (metrics.accuracy_score(y_train, y_train_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ok, not optimal, but we got a baseline. Lets pickle the SGD model and try something a little more complex \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the Model to file in the current working directory\n",
    "Pkl_Filename = \"Baseline_SGD_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(text_clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The pickled model is too big! Have to break up the pipeline and pickle individually\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This didn't work, but an attempt was made so we'll leave it in here\n",
    "#def picklizer(to_pickle, filename, path):\n",
    "    \"\"\"\n",
    "    Creates a pickle file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    to_pickle : Python object\n",
    "        The trained / fitted instance of the \n",
    "        transformer or model to be pickled.\n",
    "    filename : string\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "    path : string or path-like object\n",
    "        The path to the desired output directory.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    # Create the path to save location\n",
    "    picklepath = os.path.join(path, filename)\n",
    "\n",
    "    # Use context manager to open file\n",
    "    with open(picklepath, \"wb\") as p:\n",
    "        pickle.dump(to_pickle, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect = text_clf.named_steps['vect']\n",
    "#tfidf = text_clf.named_steps['tfidf']\n",
    "#clf = text_clf.named_steps['clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = \"models/\"\n",
    "\n",
    "# Export vectorizer as pickle\n",
    "#picklizer(vect, \"vec_01.pkl\", filepath)\n",
    "\n",
    "# Export transformer as pickle\n",
    "#picklizer(tfidf, \"tfidf_01.pkl\", filepath)\n",
    "\n",
    "# Export sgd model as pickle\n",
    "#picklizer(clf, \"clf_01.pkl\", filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now lets load the pickled model and try to run predictions\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Baseline_SGD_Model = pickle.load(file)\n",
    "\n",
    "Baseline_SGD_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Baseline_SGD_Model.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([329])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Baseline_SGD_Model.predict(test_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5342546890424482\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_train_pred = Baseline_SGD_Model.predict(X_train)\n",
    "print (metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The only way we could get the pickled model on github was compressing the file with 7zip, lets try to unzip it \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py7zr\n",
      "  Downloading py7zr-0.7.4-py3-none-any.whl (60 kB)\n",
      "Collecting texttable\n",
      "  Downloading texttable-1.6.2-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.9.8-cp37-cp37m-win_amd64.whl (14.1 MB)\n",
      "Installing collected packages: texttable, pycryptodome, py7zr\n",
      "Successfully installed py7zr-0.7.4 pycryptodome-3.9.8 texttable-1.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script py7zr.exe is installed in 'E:\\Anaconda\\envs\\U4-S1-NLP\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "\n",
    "archive = py7zr.SevenZipFile('Baseline_SGD_Model.7z', mode='r')\n",
    "archive.extractall(path=\"/tmp\")\n",
    "archive.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Baseline TFIDFV + NN / RF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
