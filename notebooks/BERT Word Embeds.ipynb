{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Formatting for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT has several requirements for it's input.  \n",
    "\n",
    "\n",
    "3 .tsv files are required.\n",
    "- train.tsv (no header)\n",
    "- dev.tsv (evaluation, no header)\n",
    "- test.tsv (header is required)  \n",
    "\n",
    "For *train.tsv* and *dev.tsv*, 4 columns must exist\n",
    "1. Column 1: The guid. Any value that is observation unique\n",
    "2. Column 2: The label. This is a string, but numbers can be used as well\n",
    "3. Column 3: Untokenized text of second sequence if doing sequence pair tasks (optional)\n",
    "4. Column 4: Untokenized text of first sequence (required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean html tags\n",
    "def soup(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(filename):\n",
    "    \"\"\"Takes in a filename (string) and returns a cleaned dataset\"\"\"\n",
    "    # Import the dataset\n",
    "    df_full = pd.read_csv(filename, sep='\\t')\n",
    "    print(\"Original Size:\", df_full.shape)\n",
    "    \n",
    "    # Concatenate title to post body text\n",
    "    df_full[\"full_text\"] = df_full[\"title\"] + \" \" + df_full[\"selftext\"]\n",
    "    \n",
    "    # Extract 1/10 of the dataset \n",
    "    df = df_full.sample(frac=0.1, random_state=7)\n",
    "    print(\"Sampled Size:\", df.shape)\n",
    "    \n",
    "    # Clean web tags and artifacts\n",
    "    print(\"Cleaning text...\")\n",
    "    df['clean_text'] = df['full_text'].apply(soup)\n",
    "    \n",
    "    # Drop columns, sort by index to prepare for encode\n",
    "    df = df[['id','subreddit','clean_text']]\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(column):\n",
    "    \"\"\"Encodes target labels and returns a dictionary\n",
    "    Param: Dataframe column\n",
    "        Example: df['column']\n",
    "    Outputs: labels_encoded, name_mapping\n",
    "        labels_encoded is an np.ndarray that can be appended to dataframe\n",
    "        name_mapping is a matched dictionary of the generated encodings to labels\n",
    "    \"\"\"\n",
    "    # Instantiate encoder\n",
    "    print(\"Encoding Labels...\")\n",
    "    encoder = LabelEncoder()\n",
    "    labels_encoded = encoder.fit_transform(column)\n",
    "    \n",
    "    # Make dictionary of labels\n",
    "    name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    \n",
    "    return labels_encoded, name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_datasets(df, labels_encoded):\n",
    "    \"\"\"Applies label encodings and returns 3 datasets formatted for BERT\n",
    "    Params: df, the dataframe to append label encodings to\n",
    "        labels_encoded, the encodings\n",
    "    Output: train, dev, test \n",
    "        Three dataframes\n",
    "    \n",
    "    \"\"\"\n",
    "    df['labels'] = labels_encoded\n",
    "    df['SPACE'] = 'a'\n",
    "    df = df[['id','labels','SPACE','clean_text']]\n",
    "    \n",
    "    # Need a train, validate, test split for BERT\n",
    "    train, dev = train_test_split(df, test_size=0.15, stratify=df[\"labels\"])\n",
    "    train, test = train_test_split(train, test_size=0.15, stratify=train[\"labels\"])\n",
    "    \n",
    "    # Fix test\n",
    "    test = test[['id','clean_text']]\n",
    "    test = test.rename(columns={\"id\": \"guid\", \"clean_text\": \"text\"})\n",
    "    \n",
    "    print(\"Datasets Created.\")\n",
    "\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = clean_dataset('reddit_posts.tsv')\n",
    "    labels_encoded, name_mapping = encode_labels(df['subreddit'])\n",
    "    train, dev, test = new_datasets(df, labels_encoded)\n",
    "    \n",
    "    #output tsv file, no header for train and dev\n",
    "    train.to_csv('dataset/train.tsv', sep='\\t', index=False, header=False)\n",
    "    dev.to_csv('dataset/dev.tsv', sep='\\t', index=False, header=False)\n",
    "    test.to_csv('dataset/test.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size: (1013000, 4)\n",
      "Sampled Size: (101300, 5)\n",
      "Cleaning text...\n",
      "Encoding Labels...\n",
      "Datasets Created.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
